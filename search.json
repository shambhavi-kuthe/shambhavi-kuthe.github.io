[
  {
    "objectID": "posts/Blog 3/index.html",
    "href": "posts/Blog 3/index.html",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "Machine learning comprises a variety of algorithms and strategies for producing data-driven predictions or choices. The purpose of regression in this discipline is to model the connection between input data and a target variable. Linear and non-linear regression are the two basic forms, each suited to distinct sorts of data correlations.\n\n\nLinear regression is a simple and extensively used approach for modeling the connection between one or more independent variables (features) and a dependent variable (output). Fitting a linear equation to observed data is the essence of linear regression. This equation is as follows:\n\\[ Y=β0+β1X1+β2X2+…+βnXn+ϵ\\]\nHere:\n\nY is the dependent variable\nX1, X2,…,Xn are the independent variables,\nβ0, β1,…,βn are the coefficients representing the relationship between variables, and\nϵϵ is the error term.\n\nLinear regression assumes a linear relationship between the input and output characteristics. The coefficients are calculated using techniques such as the Least Squares approach, which minimizes the sum of the squared differences between the anticipated and actual values.\n\n\n\nIn contrast, non-linear regression recognizes that the relationship between the independent and dependent variables is not strictly linear. Data may follow a more complicated pattern in many real-world circumstances. Modeling such relationships becomes more flexible using non-linear regression.\nThe sigmoid function is a popular example of a non-linear function: \\[Y=1/1+e^{−(β1(X−β2)Y}\\]\nIn this case, β1 and β2 are parameters that must be approximated. The sigmoid function is often used when modeling binary outcomes, as it maps any real-valued number to the range (0, 1).\n\n\n\nLet’s look at a real-world example of linear regression in Python. We utilize the FuelConsumption dataset in the given code to anticipate CO2 emissions depending on car engine size. The linear regression model is trained, displayed, and validated.\nDataset:\nFuelConsumption.csv is a fuel consumption dataset that provides model-specific fuel consumption ratings and projected CO2 emissions for new light-duty cars for retail sale in Canada. The dataset is downloaded from: https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64\nThe dataset has the following data:\n\nMODELYEAR e.g. 2014\nMAKE e.g. Acura MODEL e.g. ILX\nVEHICLE CLASS e.g. SUV ENGINE\nSIZE e.g. 4.7\nCYLINDERS e.g 6\nTRANSMISSION e.g. A6\nFUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9\nFUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9\nFUEL CONSUMPTION COMB (L/100 km) e.g. 9.2\nCO2 EMISSIONS (g/km) e.g. 182 –&gt; low –&gt; 0\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\n\n# Loading the dataset\ndf = pd.read_csv('Data\\FuelConsumption.csv')\ndf.head()\n\n# Selecting relevant column\ncdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]\ncdf\n\n\n\n\n\n\n\n\nENGINESIZE\nCYLINDERS\nFUELCONSUMPTION_COMB\nCO2EMISSIONS\n\n\n\n\n0\n2.0\n4\n8.5\n196\n\n\n1\n2.4\n4\n9.6\n221\n\n\n2\n1.5\n4\n5.9\n136\n\n\n3\n3.5\n6\n11.1\n255\n\n\n4\n3.5\n6\n10.6\n244\n\n\n...\n...\n...\n...\n...\n\n\n1062\n3.0\n6\n11.8\n271\n\n\n1063\n3.2\n6\n11.5\n264\n\n\n1064\n3.0\n6\n11.8\n271\n\n\n1065\n3.2\n6\n11.3\n260\n\n\n1066\n3.2\n6\n12.8\n294\n\n\n\n\n1067 rows × 4 columns\n\n\n\n\n# Visualizing the data\nplt.scatter(cdf.ENGINESIZE,cdf.CO2EMISSIONS,color='blue')\nplt.xlabel('Engine Size')\nplt.ylabel('Co2 Emissions')\nplt.show()\n\n\n\n\n\n# Splitting data into training and testing sets\nmsk = np.random.rand(len(df)) &lt;0.8\ntraining = cdf[msk]\ntesting = cdf[~msk]\n\n\n# Applying Polynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ntrainingX = np.asanyarray(training[['ENGINESIZE']])\ntrainingY = np.asanyarray(training[['CO2EMISSIONS']])\n\ntestingX = np.asanyarray(training[['ENGINESIZE']])\ntestingY = np.asanyarray(training[['CO2EMISSIONS']])\n\npoly = PolynomialFeatures(degree=3)\ntrainingX_poly = poly.fit_transform(trainingX)\n\nclf = LinearRegression()\ntrainingY = clf.fit(trainingX_poly,trainingY)\n\nprint('Coefficient - {} \\n Intercept - {}'.format(clf.coef_,clf.intercept_))\n\nCoefficient - [[ 0.         28.64175088  4.53199322 -0.51235786]] \n Intercept - [130.9310119]\n\n\n\n# Visualizing the data and the regression line\nplt.scatter(training['ENGINESIZE'],training['CO2EMISSIONS'],color='blue')\n\nxx = np.arange(0.0,10.0,0.1)\nyy = clf.intercept_[0]+clf.coef_[0][1]*xx+clf.coef_[0][2]*np.power(xx,2)\n\nplt.plot(xx,yy,'-r')\nplt.xlabel('Engine Size')\nplt.ylabel('Co2 Emission')\n\nplt.show()\n\n\n\n\n\n# Evaluating model accuracy\nfrom sklearn.metrics import r2_score\n\ntestingX_poly = poly.fit_transform(testingX)\ntestingY_ = clf.predict(testingX_poly)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(testingY_ - testingY)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((testingY_ - testingY) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(testingY_ , testingY) )\n\nMean absolute error: 22.91\nResidual sum of squares (MSE): 912.50\nR2-score: 0.69\n\n\nThe FuelConsumption dataset is used in this linear regression example to forecast CO2 emissions depending on engine size. The data is divided into training and testing sets, and a linear regression model is developed and assessed.\n\n\n\nAn example of non-linear regression is with GDP data from China over time. A sigmoid function is used to simulate GDP growth in this example. The curve_fit function is used to find the sigmoid function parameters, and the resultant model is plotted against the original data points.\n\nimport numpy as np\nimport pandas as pd\n \n# Loading the dataset   \ndf = pd.read_csv(\"Data/china_gdp.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n5\n1965\n6.970915e+10\n\n\n6\n1966\n7.587943e+10\n\n\n7\n1967\n7.205703e+10\n\n\n8\n1968\n6.999350e+10\n\n\n9\n1969\n7.871882e+10\n\n\n\n\n\n\n\n\n# Normalizing data\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nx_data, y_data\n\n(array([1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970,\n        1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981,\n        1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n        1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n        2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014],\n       dtype=int64),\n array([5.91841165e+10, 4.95570502e+10, 4.66851785e+10, 5.00973033e+10,\n        5.90622549e+10, 6.97091531e+10, 7.58794348e+10, 7.20570286e+10,\n        6.99934979e+10, 7.87188205e+10, 9.15062113e+10, 9.85620238e+10,\n        1.12159814e+11, 1.36769878e+11, 1.42254742e+11, 1.61162492e+11,\n        1.51627687e+11, 1.72349014e+11, 1.48382112e+11, 1.76856525e+11,\n        1.89649992e+11, 1.94369049e+11, 2.03549627e+11, 2.28950201e+11,\n        2.58082147e+11, 3.07479586e+11, 2.98805793e+11, 2.71349773e+11,\n        3.10722214e+11, 3.45957486e+11, 3.58973230e+11, 3.81454704e+11,\n        4.24934066e+11, 4.42874596e+11, 5.62261130e+11, 7.32032045e+11,\n        8.60844098e+11, 9.58159425e+11, 1.02527690e+12, 1.08944711e+12,\n        1.20526068e+12, 1.33223472e+12, 1.46190649e+12, 1.64992872e+12,\n        1.94174560e+12, 2.26859890e+12, 2.72978403e+12, 3.52309431e+12,\n        4.55843107e+12, 5.05941974e+12, 6.03965851e+12, 7.49243210e+12,\n        8.46162316e+12, 9.49060260e+12, 1.03548317e+13]))\n\n\n\n# Visualizing the dataset\nplt.scatter(x_data,y_data,color='blue')\nplt.xlabel('Year')\nplt.ylabel('GDP Values')\nplt.show()\n\n\n\n\n\nxdata = x_data/max(x_data)\nydata = y_data/max(y_data)\n\nxdata , ydata\n\n(array([0.97318769, 0.97368421, 0.97418073, 0.97467726, 0.97517378,\n        0.97567031, 0.97616683, 0.97666336, 0.97715988, 0.97765641,\n        0.97815293, 0.97864945, 0.97914598, 0.9796425 , 0.98013903,\n        0.98063555, 0.98113208, 0.9816286 , 0.98212512, 0.98262165,\n        0.98311817, 0.9836147 , 0.98411122, 0.98460775, 0.98510427,\n        0.98560079, 0.98609732, 0.98659384, 0.98709037, 0.98758689,\n        0.98808342, 0.98857994, 0.98907646, 0.98957299, 0.99006951,\n        0.99056604, 0.99106256, 0.99155909, 0.99205561, 0.99255214,\n        0.99304866, 0.99354518, 0.99404171, 0.99453823, 0.99503476,\n        0.99553128, 0.99602781, 0.99652433, 0.99702085, 0.99751738,\n        0.9980139 , 0.99851043, 0.99900695, 0.99950348, 1.        ]),\n array([0.0057156 , 0.00478589, 0.00450854, 0.00483806, 0.00570384,\n        0.00673204, 0.00732793, 0.00695878, 0.0067595 , 0.00760213,\n        0.00883705, 0.00951846, 0.01083164, 0.01320831, 0.01373801,\n        0.01556399, 0.01464318, 0.01664431, 0.01432975, 0.01707961,\n        0.01831512, 0.01877086, 0.01965745, 0.02211047, 0.02492384,\n        0.02969431, 0.02885665, 0.02620514, 0.03000746, 0.03341025,\n        0.03466722, 0.03683833, 0.04103727, 0.04276985, 0.0542994 ,\n        0.07069473, 0.08313453, 0.09253259, 0.09901435, 0.10521147,\n        0.11639597, 0.12865827, 0.1411811 , 0.15933902, 0.18752073,\n        0.21908602, 0.26362418, 0.34023675, 0.44022261, 0.48860473,\n        0.58326959, 0.7235687 , 0.81716665, 0.91653856, 1.        ]))\n\n\n\n# Defining a sigmoid function\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n\n# Initial prediction with sigmoid function\nbeta_1 = 0.10\nbeta_2 = 1990.0\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n# Determining values of beta_1 and beta_2 using curve_fit\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\nplt.show()\n\n\n\n\nApparantly it doesnt fit the data\n\n# Lets normalize our data\nxdata = x_data/max(x_data)\nydata = y_data/max(y_data)\n\nxdata , ydata\n\n(array([0.97318769, 0.97368421, 0.97418073, 0.97467726, 0.97517378,\n        0.97567031, 0.97616683, 0.97666336, 0.97715988, 0.97765641,\n        0.97815293, 0.97864945, 0.97914598, 0.9796425 , 0.98013903,\n        0.98063555, 0.98113208, 0.9816286 , 0.98212512, 0.98262165,\n        0.98311817, 0.9836147 , 0.98411122, 0.98460775, 0.98510427,\n        0.98560079, 0.98609732, 0.98659384, 0.98709037, 0.98758689,\n        0.98808342, 0.98857994, 0.98907646, 0.98957299, 0.99006951,\n        0.99056604, 0.99106256, 0.99155909, 0.99205561, 0.99255214,\n        0.99304866, 0.99354518, 0.99404171, 0.99453823, 0.99503476,\n        0.99553128, 0.99602781, 0.99652433, 0.99702085, 0.99751738,\n        0.9980139 , 0.99851043, 0.99900695, 0.99950348, 1.        ]),\n array([0.0057156 , 0.00478589, 0.00450854, 0.00483806, 0.00570384,\n        0.00673204, 0.00732793, 0.00695878, 0.0067595 , 0.00760213,\n        0.00883705, 0.00951846, 0.01083164, 0.01320831, 0.01373801,\n        0.01556399, 0.01464318, 0.01664431, 0.01432975, 0.01707961,\n        0.01831512, 0.01877086, 0.01965745, 0.02211047, 0.02492384,\n        0.02969431, 0.02885665, 0.02620514, 0.03000746, 0.03341025,\n        0.03466722, 0.03683833, 0.04103727, 0.04276985, 0.0542994 ,\n        0.07069473, 0.08313453, 0.09253259, 0.09901435, 0.10521147,\n        0.11639597, 0.12865827, 0.1411811 , 0.15933902, 0.18752073,\n        0.21908602, 0.26362418, 0.34023675, 0.44022261, 0.48860473,\n        0.58326959, 0.7235687 , 0.81716665, 0.91653856, 1.        ]))\n\n\n\n# We need to determine the values of beta_1 and beta_2\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451712, beta_2 = 0.997207\n\n\n\n# Plotting the regression model\nx = np.linspace(1960,2015,55)\nx = x/max(x)\n\ny = sigmoid(x,*popt)\n\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y,linewidth=3.0,label='fit')\nplt.legend(loc='best')\n\nplt.xlabel('Year')\nplt.ylabel('Values')\n\nplt.show()\n\n\n\n\nWe utilize a sigmoid function to simulate China’s GDP growth over time in this non-linear regression example. The data is normalized, and the curve_fit function is used to find the best sigmoid function parameters. The generated model is then compared to the original data points.\n\n\n\nFinally, both linear and non-linear regression play important roles in machine learning, catering to different types of data correlations. Linear regression is appropriate for linear connections, but non-linear regression is more versatile for capturing complicated patterns. The decision relies on the type of the data as well as the underlying connections to be modeled. Python and its libraries, like as NumPy and scikit-learn, provide strong tools for creating and visualizing regression models, as we’ve shown."
  },
  {
    "objectID": "posts/Blog 3/index.html#linear-regression",
    "href": "posts/Blog 3/index.html#linear-regression",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "Linear regression is a simple and extensively used approach for modeling the connection between one or more independent variables (features) and a dependent variable (output). Fitting a linear equation to observed data is the essence of linear regression. This equation is as follows:\n\\[ Y=β0+β1X1+β2X2+…+βnXn+ϵ\\]\nHere:\n\nY is the dependent variable\nX1, X2,…,Xn are the independent variables,\nβ0, β1,…,βn are the coefficients representing the relationship between variables, and\nϵϵ is the error term.\n\nLinear regression assumes a linear relationship between the input and output characteristics. The coefficients are calculated using techniques such as the Least Squares approach, which minimizes the sum of the squared differences between the anticipated and actual values."
  },
  {
    "objectID": "posts/Blog 3/index.html#non-linear-regression",
    "href": "posts/Blog 3/index.html#non-linear-regression",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "In contrast, non-linear regression recognizes that the relationship between the independent and dependent variables is not strictly linear. Data may follow a more complicated pattern in many real-world circumstances. Modeling such relationships becomes more flexible using non-linear regression.\nThe sigmoid function is a popular example of a non-linear function: \\[Y=1/1+e^{−(β1(X−β2)Y}\\]\nIn this case, β1 and β2 are parameters that must be approximated. The sigmoid function is often used when modeling binary outcomes, as it maps any real-valued number to the range (0, 1)."
  },
  {
    "objectID": "posts/Blog 3/index.html#the-code-breakdown-linear-regression",
    "href": "posts/Blog 3/index.html#the-code-breakdown-linear-regression",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "Let’s look at a real-world example of linear regression in Python. We utilize the FuelConsumption dataset in the given code to anticipate CO2 emissions depending on car engine size. The linear regression model is trained, displayed, and validated.\nDataset:\nFuelConsumption.csv is a fuel consumption dataset that provides model-specific fuel consumption ratings and projected CO2 emissions for new light-duty cars for retail sale in Canada. The dataset is downloaded from: https://open.canada.ca/data/en/dataset/98f1a129-f628-4ce4-b24d-6f16bf24dd64\nThe dataset has the following data:\n\nMODELYEAR e.g. 2014\nMAKE e.g. Acura MODEL e.g. ILX\nVEHICLE CLASS e.g. SUV ENGINE\nSIZE e.g. 4.7\nCYLINDERS e.g 6\nTRANSMISSION e.g. A6\nFUEL CONSUMPTION in CITY(L/100 km) e.g. 9.9\nFUEL CONSUMPTION in HWY (L/100 km) e.g. 8.9\nFUEL CONSUMPTION COMB (L/100 km) e.g. 9.2\nCO2 EMISSIONS (g/km) e.g. 182 –&gt; low –&gt; 0\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\n\n# Loading the dataset\ndf = pd.read_csv('Data\\FuelConsumption.csv')\ndf.head()\n\n# Selecting relevant column\ncdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]\ncdf\n\n\n\n\n\n\n\n\nENGINESIZE\nCYLINDERS\nFUELCONSUMPTION_COMB\nCO2EMISSIONS\n\n\n\n\n0\n2.0\n4\n8.5\n196\n\n\n1\n2.4\n4\n9.6\n221\n\n\n2\n1.5\n4\n5.9\n136\n\n\n3\n3.5\n6\n11.1\n255\n\n\n4\n3.5\n6\n10.6\n244\n\n\n...\n...\n...\n...\n...\n\n\n1062\n3.0\n6\n11.8\n271\n\n\n1063\n3.2\n6\n11.5\n264\n\n\n1064\n3.0\n6\n11.8\n271\n\n\n1065\n3.2\n6\n11.3\n260\n\n\n1066\n3.2\n6\n12.8\n294\n\n\n\n\n1067 rows × 4 columns\n\n\n\n\n# Visualizing the data\nplt.scatter(cdf.ENGINESIZE,cdf.CO2EMISSIONS,color='blue')\nplt.xlabel('Engine Size')\nplt.ylabel('Co2 Emissions')\nplt.show()\n\n\n\n\n\n# Splitting data into training and testing sets\nmsk = np.random.rand(len(df)) &lt;0.8\ntraining = cdf[msk]\ntesting = cdf[~msk]\n\n\n# Applying Polynomial Regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\ntrainingX = np.asanyarray(training[['ENGINESIZE']])\ntrainingY = np.asanyarray(training[['CO2EMISSIONS']])\n\ntestingX = np.asanyarray(training[['ENGINESIZE']])\ntestingY = np.asanyarray(training[['CO2EMISSIONS']])\n\npoly = PolynomialFeatures(degree=3)\ntrainingX_poly = poly.fit_transform(trainingX)\n\nclf = LinearRegression()\ntrainingY = clf.fit(trainingX_poly,trainingY)\n\nprint('Coefficient - {} \\n Intercept - {}'.format(clf.coef_,clf.intercept_))\n\nCoefficient - [[ 0.         28.64175088  4.53199322 -0.51235786]] \n Intercept - [130.9310119]\n\n\n\n# Visualizing the data and the regression line\nplt.scatter(training['ENGINESIZE'],training['CO2EMISSIONS'],color='blue')\n\nxx = np.arange(0.0,10.0,0.1)\nyy = clf.intercept_[0]+clf.coef_[0][1]*xx+clf.coef_[0][2]*np.power(xx,2)\n\nplt.plot(xx,yy,'-r')\nplt.xlabel('Engine Size')\nplt.ylabel('Co2 Emission')\n\nplt.show()\n\n\n\n\n\n# Evaluating model accuracy\nfrom sklearn.metrics import r2_score\n\ntestingX_poly = poly.fit_transform(testingX)\ntestingY_ = clf.predict(testingX_poly)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(testingY_ - testingY)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((testingY_ - testingY) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(testingY_ , testingY) )\n\nMean absolute error: 22.91\nResidual sum of squares (MSE): 912.50\nR2-score: 0.69\n\n\nThe FuelConsumption dataset is used in this linear regression example to forecast CO2 emissions depending on engine size. The data is divided into training and testing sets, and a linear regression model is developed and assessed."
  },
  {
    "objectID": "posts/Blog 3/index.html#the-code-breakdown-non-linear-regression",
    "href": "posts/Blog 3/index.html#the-code-breakdown-non-linear-regression",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "An example of non-linear regression is with GDP data from China over time. A sigmoid function is used to simulate GDP growth in this example. The curve_fit function is used to find the sigmoid function parameters, and the resultant model is plotted against the original data points.\n\nimport numpy as np\nimport pandas as pd\n \n# Loading the dataset   \ndf = pd.read_csv(\"Data/china_gdp.csv\")\ndf.head(10)\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n5\n1965\n6.970915e+10\n\n\n6\n1966\n7.587943e+10\n\n\n7\n1967\n7.205703e+10\n\n\n8\n1968\n6.999350e+10\n\n\n9\n1969\n7.871882e+10\n\n\n\n\n\n\n\n\n# Normalizing data\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nx_data, y_data\n\n(array([1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970,\n        1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981,\n        1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992,\n        1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n        2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014],\n       dtype=int64),\n array([5.91841165e+10, 4.95570502e+10, 4.66851785e+10, 5.00973033e+10,\n        5.90622549e+10, 6.97091531e+10, 7.58794348e+10, 7.20570286e+10,\n        6.99934979e+10, 7.87188205e+10, 9.15062113e+10, 9.85620238e+10,\n        1.12159814e+11, 1.36769878e+11, 1.42254742e+11, 1.61162492e+11,\n        1.51627687e+11, 1.72349014e+11, 1.48382112e+11, 1.76856525e+11,\n        1.89649992e+11, 1.94369049e+11, 2.03549627e+11, 2.28950201e+11,\n        2.58082147e+11, 3.07479586e+11, 2.98805793e+11, 2.71349773e+11,\n        3.10722214e+11, 3.45957486e+11, 3.58973230e+11, 3.81454704e+11,\n        4.24934066e+11, 4.42874596e+11, 5.62261130e+11, 7.32032045e+11,\n        8.60844098e+11, 9.58159425e+11, 1.02527690e+12, 1.08944711e+12,\n        1.20526068e+12, 1.33223472e+12, 1.46190649e+12, 1.64992872e+12,\n        1.94174560e+12, 2.26859890e+12, 2.72978403e+12, 3.52309431e+12,\n        4.55843107e+12, 5.05941974e+12, 6.03965851e+12, 7.49243210e+12,\n        8.46162316e+12, 9.49060260e+12, 1.03548317e+13]))\n\n\n\n# Visualizing the dataset\nplt.scatter(x_data,y_data,color='blue')\nplt.xlabel('Year')\nplt.ylabel('GDP Values')\nplt.show()\n\n\n\n\n\nxdata = x_data/max(x_data)\nydata = y_data/max(y_data)\n\nxdata , ydata\n\n(array([0.97318769, 0.97368421, 0.97418073, 0.97467726, 0.97517378,\n        0.97567031, 0.97616683, 0.97666336, 0.97715988, 0.97765641,\n        0.97815293, 0.97864945, 0.97914598, 0.9796425 , 0.98013903,\n        0.98063555, 0.98113208, 0.9816286 , 0.98212512, 0.98262165,\n        0.98311817, 0.9836147 , 0.98411122, 0.98460775, 0.98510427,\n        0.98560079, 0.98609732, 0.98659384, 0.98709037, 0.98758689,\n        0.98808342, 0.98857994, 0.98907646, 0.98957299, 0.99006951,\n        0.99056604, 0.99106256, 0.99155909, 0.99205561, 0.99255214,\n        0.99304866, 0.99354518, 0.99404171, 0.99453823, 0.99503476,\n        0.99553128, 0.99602781, 0.99652433, 0.99702085, 0.99751738,\n        0.9980139 , 0.99851043, 0.99900695, 0.99950348, 1.        ]),\n array([0.0057156 , 0.00478589, 0.00450854, 0.00483806, 0.00570384,\n        0.00673204, 0.00732793, 0.00695878, 0.0067595 , 0.00760213,\n        0.00883705, 0.00951846, 0.01083164, 0.01320831, 0.01373801,\n        0.01556399, 0.01464318, 0.01664431, 0.01432975, 0.01707961,\n        0.01831512, 0.01877086, 0.01965745, 0.02211047, 0.02492384,\n        0.02969431, 0.02885665, 0.02620514, 0.03000746, 0.03341025,\n        0.03466722, 0.03683833, 0.04103727, 0.04276985, 0.0542994 ,\n        0.07069473, 0.08313453, 0.09253259, 0.09901435, 0.10521147,\n        0.11639597, 0.12865827, 0.1411811 , 0.15933902, 0.18752073,\n        0.21908602, 0.26362418, 0.34023675, 0.44022261, 0.48860473,\n        0.58326959, 0.7235687 , 0.81716665, 0.91653856, 1.        ]))\n\n\n\n# Defining a sigmoid function\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n\n# Initial prediction with sigmoid function\nbeta_1 = 0.10\nbeta_2 = 1990.0\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n# Determining values of beta_1 and beta_2 using curve_fit\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\nplt.show()\n\n\n\n\nApparantly it doesnt fit the data\n\n# Lets normalize our data\nxdata = x_data/max(x_data)\nydata = y_data/max(y_data)\n\nxdata , ydata\n\n(array([0.97318769, 0.97368421, 0.97418073, 0.97467726, 0.97517378,\n        0.97567031, 0.97616683, 0.97666336, 0.97715988, 0.97765641,\n        0.97815293, 0.97864945, 0.97914598, 0.9796425 , 0.98013903,\n        0.98063555, 0.98113208, 0.9816286 , 0.98212512, 0.98262165,\n        0.98311817, 0.9836147 , 0.98411122, 0.98460775, 0.98510427,\n        0.98560079, 0.98609732, 0.98659384, 0.98709037, 0.98758689,\n        0.98808342, 0.98857994, 0.98907646, 0.98957299, 0.99006951,\n        0.99056604, 0.99106256, 0.99155909, 0.99205561, 0.99255214,\n        0.99304866, 0.99354518, 0.99404171, 0.99453823, 0.99503476,\n        0.99553128, 0.99602781, 0.99652433, 0.99702085, 0.99751738,\n        0.9980139 , 0.99851043, 0.99900695, 0.99950348, 1.        ]),\n array([0.0057156 , 0.00478589, 0.00450854, 0.00483806, 0.00570384,\n        0.00673204, 0.00732793, 0.00695878, 0.0067595 , 0.00760213,\n        0.00883705, 0.00951846, 0.01083164, 0.01320831, 0.01373801,\n        0.01556399, 0.01464318, 0.01664431, 0.01432975, 0.01707961,\n        0.01831512, 0.01877086, 0.01965745, 0.02211047, 0.02492384,\n        0.02969431, 0.02885665, 0.02620514, 0.03000746, 0.03341025,\n        0.03466722, 0.03683833, 0.04103727, 0.04276985, 0.0542994 ,\n        0.07069473, 0.08313453, 0.09253259, 0.09901435, 0.10521147,\n        0.11639597, 0.12865827, 0.1411811 , 0.15933902, 0.18752073,\n        0.21908602, 0.26362418, 0.34023675, 0.44022261, 0.48860473,\n        0.58326959, 0.7235687 , 0.81716665, 0.91653856, 1.        ]))\n\n\n\n# We need to determine the values of beta_1 and beta_2\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451712, beta_2 = 0.997207\n\n\n\n# Plotting the regression model\nx = np.linspace(1960,2015,55)\nx = x/max(x)\n\ny = sigmoid(x,*popt)\n\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y,linewidth=3.0,label='fit')\nplt.legend(loc='best')\n\nplt.xlabel('Year')\nplt.ylabel('Values')\n\nplt.show()\n\n\n\n\nWe utilize a sigmoid function to simulate China’s GDP growth over time in this non-linear regression example. The data is normalized, and the curve_fit function is used to find the best sigmoid function parameters. The generated model is then compared to the original data points."
  },
  {
    "objectID": "posts/Blog 3/index.html#conclusion",
    "href": "posts/Blog 3/index.html#conclusion",
    "title": "Blog 3: Linear and Non-linear Regression",
    "section": "",
    "text": "Finally, both linear and non-linear regression play important roles in machine learning, catering to different types of data correlations. Linear regression is appropriate for linear connections, but non-linear regression is more versatile for capturing complicated patterns. The decision relies on the type of the data as well as the underlying connections to be modeled. Python and its libraries, like as NumPy and scikit-learn, provide strong tools for creating and visualizing regression models, as we’ve shown."
  },
  {
    "objectID": "posts/Blog 1/index.html",
    "href": "posts/Blog 1/index.html",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "Probability Theory\nProbability theory is a fundamental building block of machine learning, offering a mathematical framework for modeling uncertainty and quantifying the likelihood of various events. It helps us to represent and modify uncertainty associated with predictions and judgments in machine learning. Probability distributions, such as Gaussian distributions, are widely used to depict the chance of occurrence of certain occurrences. Understanding probability theory is critical for developing strong machine learning models capable of dealing with and interpreting uncertainty in real-world data.\nRandom Variables\nRandom variables, which represent uncertain quantities that might take on multiple values, are a key notion in probability theory and machine learning. Features and outcomes are frequently handled as random variables in the context of machine learning. The study of random variables allows for the modeling of uncertain events, assisting data scientists and machine learning practitioners in making educated judgments based on data’s intrinsic unpredictability. Random variables are essential for issue formulation and resolution in predictive modeling and statistical inference.\nGaussian Naive Bayes Model\nThe Gaussian Naive Bayes model is a probabilistic classification technique that makes predictions using Bayes’ theorem. It is especially beneficial in machine learning problems where features are believed to have a Gaussian (normal) distribution. The “naive” part of the model relates to the assumption of independence between features, which simplifies probability computation. This model is very useful for text categorization and is a common choice when working with continuous data. Understanding the ideas underlying the Gaussian Naive Bayes model is useful for machine learning practitioners who want to create efficient and interpretable classification systems.\n\n\nTo give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate.\n\n\n\nIn conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "posts/Blog 1/index.html#the-code-breakdown",
    "href": "posts/Blog 1/index.html#the-code-breakdown",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "To give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate."
  },
  {
    "objectID": "posts/Blog 1/index.html#conclusion",
    "href": "posts/Blog 1/index.html#conclusion",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "In conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "about.html#course-cs-5805-machine-learning",
    "href": "about.html#course-cs-5805-machine-learning",
    "title": "About",
    "section": "Course: CS 5805 Machine Learning",
    "text": "Course: CS 5805 Machine Learning\nThis is a five series blog on the following Machine Learning topics for the course CS 5805 at Virginia tech.\nTopics:\n\nProbability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/outlier detection"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Blog 1: Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2: Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3: Linear and Non-linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4: Classification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5: Anomaly/outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "Clustering, a process that involves grouping related data points together, is one of the fundamental jobs in the large area of machine learning. Clustering is important in a variety of applications, from customer segmentation to image analysis. The K-Means algorithm, recognized for its simplicity and efficacy, is a strong clustering technique.\nUnderstanding Clustering\nClustering is a fundamental machine learning approach that involves grouping together comparable data points based on underlying patterns or similarities in the data. Clustering is classified as unsupervised learning since it acts on unlabeled datasets, as opposed to supervised learning, where the algorithm is taught on labeled data. Clustering’s primary purpose is to find and arrange data points into discrete clusters, with each cluster comprising components with similar properties. This approach aids in the discovery of hidden structures inside data, making it a useful tool for tasks such as customer segmentation, anomaly detection, and pattern identification. K-Means is a popular clustering technique that divides data into K-groups and assigns each data point to the cluster with the closest centroid. Clustering algorithms play an important role in exploratory data analysis, helping to extract useful insights from vast and complicated datasets.\nK-means Clustering\nK-Means clustering is a well-known unsupervised machine learning approach for partitioning a dataset into K separate, non-overlapping subsets or clusters. Iteratively, the method assigns each data point to the cluster with the closest centroid (mean point), then updates the centroids depending on the newly allocated points. The number of clusters, K, is a user-specified parameter. K-Means is especially successful in situations where the data shows clear patterns of separation, and its simplicity and effectiveness make it popular for applications like customer segmentation, picture reduction, and anomaly identification. The algorithm’s success is due to its ability to rapidly converge on a solution, which makes it computationally efficient for huge datasets. It is, however, sensitive to the initial placement of centroids, and the value of K can have a substantial influence on the quality of the clustering results.\n\n\nLet’s look at the Python code that shows how to use K-Means clustering on customer data. The dataset utilized in this case is from a shopping mall, and the goal is to classify clients based on their yearly income and age.\nDataset:\nThe dataset was obtained from Kaggle : https://www.kaggle.com/nelakurthisudheer/mall-customer-segmentation\nThe dataset contains the following five characteristics of 200 customers :\n\nCustomerID: A unique identifier issued to the customer.\nGender: The customer’s gender\nAge: The customer’s age.\nAnnual Income (k$): The customer’s annual income. The mall assigns a spending score (1-100) based on client behavior and spending habits.\n\n\n# Importing libraries and reading the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading the dataset\ncustomers = pd.read_csv('data\\Mall_Customers.csv')\nprint(customers.head()) \n\n# Checking the summary statistics\ncustomers.describe()\n\n   CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Annual Income (k$)'])\n\n&lt;Axes: xlabel='Annual Income (k$)', ylabel='Density'&gt;\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Age'])\n\n&lt;Axes: xlabel='Age', ylabel='Density'&gt;\n\n\n\n\n\nImporting the appropriate libraries, loading the dataset, and studying its fundamental statistics and feature distributions are the first stages.\n\n\nThe elbow method is used to estimate the optimal number of clusters in the first clustering job, which is dependent on yearly income.\n\ndata = customers.iloc[:,[3,4]].values\nsos = []#within cluster sum of squares\n\n# Applying K-Means with different cluster numbers\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)# inertia_ = to find the sos value\n\n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method assists in determining the optimal number of clusters by noting when the within-cluster sum of squares (SOS) begins to plateau. The ‘elbow’ appears at k=5, suggesting that there are five unique clusters in this example.\n\n# Applying K-Means with k=5\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n#plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\nax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100, c='green', label='Cluster 5')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nThe above code segment depicts the clusters and centroids found based on yearly income and spending score.\n\n\n\nThe same process is repeated for clustering based on age.\n\n# Extracting relevant data\ndata=customers.iloc[:,[2,4]].values\n\n# Elbow method for determining optimal clusters\nsos = []\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)\n    \n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method is used once more to determine the ideal number of clusters. In this example, the code suggests that k=4 is a good choice.\n\n# Applying K-Means with k=4\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n# Plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Age')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nAge and expenditure score are used to depict clusters and centroids.\n\n\n\n\nIn this blog article, we looked at clustering, a strong unsupervised learning approach. We applied clustering to customer data using the K-Means technique, revealing separate groupings based on yearly income and age. The elbow approach was critical in finding the ideal amount of clusters, and the visualizations that resulted offered insights into consumer segmentation. Understanding and utilizing clustering techniques such as K-Means may have far-reaching consequences in a variety of fields, providing useful insights and paving the way for better informed decision-making."
  },
  {
    "objectID": "posts/Blog 2/index.html#the-code-breakdown",
    "href": "posts/Blog 2/index.html#the-code-breakdown",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "Let’s look at the Python code that shows how to use K-Means clustering on customer data. The dataset utilized in this case is from a shopping mall, and the goal is to classify clients based on their yearly income and age.\nDataset:\nThe dataset was obtained from Kaggle : https://www.kaggle.com/nelakurthisudheer/mall-customer-segmentation\nThe dataset contains the following five characteristics of 200 customers :\n\nCustomerID: A unique identifier issued to the customer.\nGender: The customer’s gender\nAge: The customer’s age.\nAnnual Income (k$): The customer’s annual income. The mall assigns a spending score (1-100) based on client behavior and spending habits.\n\n\n# Importing libraries and reading the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading the dataset\ncustomers = pd.read_csv('data\\Mall_Customers.csv')\nprint(customers.head()) \n\n# Checking the summary statistics\ncustomers.describe()\n\n   CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Annual Income (k$)'])\n\n&lt;Axes: xlabel='Annual Income (k$)', ylabel='Density'&gt;\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Age'])\n\n&lt;Axes: xlabel='Age', ylabel='Density'&gt;\n\n\n\n\n\nImporting the appropriate libraries, loading the dataset, and studying its fundamental statistics and feature distributions are the first stages.\n\n\nThe elbow method is used to estimate the optimal number of clusters in the first clustering job, which is dependent on yearly income.\n\ndata = customers.iloc[:,[3,4]].values\nsos = []#within cluster sum of squares\n\n# Applying K-Means with different cluster numbers\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)# inertia_ = to find the sos value\n\n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method assists in determining the optimal number of clusters by noting when the within-cluster sum of squares (SOS) begins to plateau. The ‘elbow’ appears at k=5, suggesting that there are five unique clusters in this example.\n\n# Applying K-Means with k=5\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n#plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\nax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100, c='green', label='Cluster 5')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nThe above code segment depicts the clusters and centroids found based on yearly income and spending score.\n\n\n\nThe same process is repeated for clustering based on age.\n\n# Extracting relevant data\ndata=customers.iloc[:,[2,4]].values\n\n# Elbow method for determining optimal clusters\nsos = []\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)\n    \n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method is used once more to determine the ideal number of clusters. In this example, the code suggests that k=4 is a good choice.\n\n# Applying K-Means with k=4\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n# Plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Age')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nAge and expenditure score are used to depict clusters and centroids."
  },
  {
    "objectID": "posts/Blog 2/index.html#conclusion",
    "href": "posts/Blog 2/index.html#conclusion",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "In this blog article, we looked at clustering, a strong unsupervised learning approach. We applied clustering to customer data using the K-Means technique, revealing separate groupings based on yearly income and age. The elbow approach was critical in finding the ideal amount of clusters, and the visualizations that resulted offered insights into consumer segmentation. Understanding and utilizing clustering techniques such as K-Means may have far-reaching consequences in a variety of fields, providing useful insights and paving the way for better informed decision-making."
  },
  {
    "objectID": "posts/Blog 4/index.html",
    "href": "posts/Blog 4/index.html",
    "title": "Blog 4: Classification",
    "section": "",
    "text": "Machine learning algorithms are critical in producing data-driven predictions and judgments. A key part of machine learning is classification, which includes assigning labels or categories to data items. Logistic Regression is a popular classification approach for binary classification issues. In this blog, we will look into Logistic Regression in depth and see how it is used for classification using a real-world dataset.\nBinary classification\nBinary classification is a machine learning task where items are categorized into two classes, typically labeled as positive and negative. The model is trained on labeled data to learn a decision boundary that separates the classes in the feature space. Key components include features, training data, and evaluation metrics like accuracy and precision. Common algorithms for binary classification include logistic regression, support vector machines, decision trees, random forests, and neural networks.\nLogistic Regression for Binary classification\nLogistic Regression is a binary classification algorithm that models the probability of an instance belonging to a particular class using the sigmoid function. It assigns weights to input features, and the model is trained to minimize the difference between predicted probabilities and actual class labels. The decision is made based on a threshold, commonly 0.5. Logistic Regression provides interpretable coefficients and is evaluated using metrics like accuracy and precision.\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load dataset\ndataset = pd.read_csv('Data/Social_Network_Ads.csv')\n\n# Extract features (x) and target variable (y)\nx = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values\n\n# Split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n\n# Feature scaling using StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nThis code snippet loads the dataset using Pandas, extracts features and the target variable, and splits the data into training and testing sets. The dataset split is performed with 25% designated for testing, and a random state is set for reproducibility. Additionally, we employ StandardScaler from scikit-learn to normalize the feature values, ensuring that the model interprets all features uniformly. This preprocessing is essential for training and evaluating machine learning models effectively, enhancing their performance on diverse datasets.\n\n# Train a Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(x_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nIn the above code segment, we import the LogisticRegression class from scikit-learn and create an instance of Logistic Regression with a random seed for reproducibility. I then use the fit method to train the model on the provided training data (x_train features and y_train labels).\n\n# Make predictions on the test set\ny_pred = classifier.predict(x_test)\n# Display the predicted and actual values side by side\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n\n[[0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 1]\n [1 1]\n [1 1]]\n\n\nWe use the trained Logistic Regression model (classifier) to predict the target variable for the test set (x_test) and concatenate the predicted values (y_pred) and actual values (y_test) side by side along the second axis (axis=1). We then print the concatenated array, showing the predicted and actual values in adjacent columns.\n\n# Evaluate the model using confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\naccuracy_score(y_test, y_pred)\n\n# Visualize the training set results\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n\n[[65  3]\n [ 8 24]]\n\n\n\n\n\n\n# Plot decision boundary on the training set\nfrom matplotlib.colors import ListedColormap\nx_set, y_set = sc.inverse_transform(x_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 10, stop = x_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = x_set[:, 1].min() - 1000, stop = x_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nWe assess the performance by evaluating its predictions on a test set. It calculates the confusion matrix and accuracy score using scikit-learn’s confusion_matrix and accuracy_score functions. The confusion matrix summarizes the model’s predictions compared to the actual labels, while the accuracy score quantifies the model’s overall correctness.\n\n# Plot decision boundary on the test set\nfrom matplotlib.colors import ListedColormap\nx_set, y_set = sc.inverse_transform(x_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 10, stop = x_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = x_set[:, 1].min() - 1000, stop = x_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nWe generates a plot illustrating a contour plot showing how the model classifies different regions. The contour plot visualizes the decision regions with distinct colors for each class. Additionally, it includes scatter points for the actual data, allowing visualization of how well the model separates the classes based on the features ‘Age’ and ‘Estimated Salary.’\n\n\n\nLogistic Regression is a very effective approach for binary classification applications. This blog has offered a step-by-step description of how to construct Logistic Regression for a dataset of social network advertising. It addressed data preparation, model training, assessment, and result visualization. Understanding and executing such algorithms are critical skills for anybody interested in machine learning. Readers may learn about logistic regression and apply their understanding to more complicated machine learning challenges by studying and playing with this code. Experimenting with different datasets and changing parameters can help one understand classification algorithms even more."
  },
  {
    "objectID": "posts/Blog 4/index.html#the-code-breakdown",
    "href": "posts/Blog 4/index.html#the-code-breakdown",
    "title": "Blog 4: Classification",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load dataset\ndataset = pd.read_csv('Data/Social_Network_Ads.csv')\n\n# Extract features (x) and target variable (y)\nx = dataset.iloc[:, 0:-1].values\ny = dataset.iloc[:, -1].values\n\n# Split the dataset into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)\n\n# Feature scaling using StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nThis code snippet loads the dataset using Pandas, extracts features and the target variable, and splits the data into training and testing sets. The dataset split is performed with 25% designated for testing, and a random state is set for reproducibility. Additionally, we employ StandardScaler from scikit-learn to normalize the feature values, ensuring that the model interprets all features uniformly. This preprocessing is essential for training and evaluating machine learning models effectively, enhancing their performance on diverse datasets.\n\n# Train a Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(x_train, y_train)\n\nLogisticRegression(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(random_state=0)\n\n\nIn the above code segment, we import the LogisticRegression class from scikit-learn and create an instance of Logistic Regression with a random seed for reproducibility. I then use the fit method to train the model on the provided training data (x_train features and y_train labels).\n\n# Make predictions on the test set\ny_pred = classifier.predict(x_test)\n# Display the predicted and actual values side by side\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))\n\n[[0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 1]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 0]\n [1 0]\n [0 0]\n [1 1]\n [1 1]\n [1 1]\n [1 0]\n [0 0]\n [0 0]\n [1 1]\n [1 1]\n [0 0]\n [1 1]\n [0 1]\n [0 0]\n [0 0]\n [1 1]\n [0 0]\n [0 0]\n [0 0]\n [0 1]\n [0 0]\n [0 1]\n [1 1]\n [1 1]]\n\n\nWe use the trained Logistic Regression model (classifier) to predict the target variable for the test set (x_test) and concatenate the predicted values (y_pred) and actual values (y_test) side by side along the second axis (axis=1). We then print the concatenated array, showing the predicted and actual values in adjacent columns.\n\n# Evaluate the model using confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\naccuracy_score(y_test, y_pred)\n\n# Visualize the training set results\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()\nplt.show()\n\n[[65  3]\n [ 8 24]]\n\n\n\n\n\n\n# Plot decision boundary on the training set\nfrom matplotlib.colors import ListedColormap\nx_set, y_set = sc.inverse_transform(x_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 10, stop = x_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = x_set[:, 1].min() - 1000, stop = x_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nWe assess the performance by evaluating its predictions on a test set. It calculates the confusion matrix and accuracy score using scikit-learn’s confusion_matrix and accuracy_score functions. The confusion matrix summarizes the model’s predictions compared to the actual labels, while the accuracy score quantifies the model’s overall correctness.\n\n# Plot decision boundary on the test set\nfrom matplotlib.colors import ListedColormap\nx_set, y_set = sc.inverse_transform(x_test), y_test\nX1, X2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 10, stop = x_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = x_set[:, 1].min() - 1000, stop = x_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n\n\n\nWe generates a plot illustrating a contour plot showing how the model classifies different regions. The contour plot visualizes the decision regions with distinct colors for each class. Additionally, it includes scatter points for the actual data, allowing visualization of how well the model separates the classes based on the features ‘Age’ and ‘Estimated Salary.’"
  },
  {
    "objectID": "posts/Blog 4/index.html#conclusion",
    "href": "posts/Blog 4/index.html#conclusion",
    "title": "Blog 4: Classification",
    "section": "",
    "text": "Logistic Regression is a very effective approach for binary classification applications. This blog has offered a step-by-step description of how to construct Logistic Regression for a dataset of social network advertising. It addressed data preparation, model training, assessment, and result visualization. Understanding and executing such algorithms are critical skills for anybody interested in machine learning. Readers may learn about logistic regression and apply their understanding to more complicated machine learning challenges by studying and playing with this code. Experimenting with different datasets and changing parameters can help one understand classification algorithms even more."
  }
]