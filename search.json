[
  {
    "objectID": "posts/Blog 1/index.html",
    "href": "posts/Blog 1/index.html",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "Probability Theory\nProbability theory is a fundamental building block of machine learning, offering a mathematical framework for modeling uncertainty and quantifying the likelihood of various events. It helps us to represent and modify uncertainty associated with predictions and judgments in machine learning. Probability distributions, such as Gaussian distributions, are widely used to depict the chance of occurrence of certain occurrences. Understanding probability theory is critical for developing strong machine learning models capable of dealing with and interpreting uncertainty in real-world data.\nRandom Variables\nRandom variables, which represent uncertain quantities that might take on multiple values, are a key notion in probability theory and machine learning. Features and outcomes are frequently handled as random variables in the context of machine learning. The study of random variables allows for the modeling of uncertain events, assisting data scientists and machine learning practitioners in making educated judgments based on data’s intrinsic unpredictability. Random variables are essential for issue formulation and resolution in predictive modeling and statistical inference.\nGaussian Naive Bayes Model\nThe Gaussian Naive Bayes model is a probabilistic classification technique that makes predictions using Bayes’ theorem. It is especially beneficial in machine learning problems where features are believed to have a Gaussian (normal) distribution. The “naive” part of the model relates to the assumption of independence between features, which simplifies probability computation. This model is very useful for text categorization and is a common choice when working with continuous data. Understanding the ideas underlying the Gaussian Naive Bayes model is useful for machine learning practitioners who want to create efficient and interpretable classification systems.\n\n\nTo give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate.\n\n\n\nIn conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "posts/Blog 1/index.html#the-code-breakdown",
    "href": "posts/Blog 1/index.html#the-code-breakdown",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "To give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate."
  },
  {
    "objectID": "posts/Blog 1/index.html#conclusion",
    "href": "posts/Blog 1/index.html#conclusion",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "In conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "about.html#course-cs-5805-machine-learning",
    "href": "about.html#course-cs-5805-machine-learning",
    "title": "About",
    "section": "Course: CS 5805 Machine Learning",
    "text": "Course: CS 5805 Machine Learning\nThis is a five series blog on the following Machine Learning topics for the course CS 5805 at Virginia tech.\nTopics:\n\nProbability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/outlier detection"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Blog 1: Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2: Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3: Linear and Non-linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4: Classification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5: Anomaly/outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\nNo matching items"
  }
]