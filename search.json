[
  {
    "objectID": "posts/Blog 1/index.html",
    "href": "posts/Blog 1/index.html",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "Probability Theory\nProbability theory is a fundamental building block of machine learning, offering a mathematical framework for modeling uncertainty and quantifying the likelihood of various events. It helps us to represent and modify uncertainty associated with predictions and judgments in machine learning. Probability distributions, such as Gaussian distributions, are widely used to depict the chance of occurrence of certain occurrences. Understanding probability theory is critical for developing strong machine learning models capable of dealing with and interpreting uncertainty in real-world data.\nRandom Variables\nRandom variables, which represent uncertain quantities that might take on multiple values, are a key notion in probability theory and machine learning. Features and outcomes are frequently handled as random variables in the context of machine learning. The study of random variables allows for the modeling of uncertain events, assisting data scientists and machine learning practitioners in making educated judgments based on data’s intrinsic unpredictability. Random variables are essential for issue formulation and resolution in predictive modeling and statistical inference.\nGaussian Naive Bayes Model\nThe Gaussian Naive Bayes model is a probabilistic classification technique that makes predictions using Bayes’ theorem. It is especially beneficial in machine learning problems where features are believed to have a Gaussian (normal) distribution. The “naive” part of the model relates to the assumption of independence between features, which simplifies probability computation. This model is very useful for text categorization and is a common choice when working with continuous data. Understanding the ideas underlying the Gaussian Naive Bayes model is useful for machine learning practitioners who want to create efficient and interpretable classification systems.\n\n\nTo give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate.\n\n\n\nIn conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "posts/Blog 1/index.html#the-code-breakdown",
    "href": "posts/Blog 1/index.html#the-code-breakdown",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "To give an example of use of probability theory in Machine learning, I demonstrate the Naive Bayes classifier technique to predict the survival of passengers on the Titanic. Using this example, we can estimate whether a given passenger would have survived the Titanic catastrophe by training a Naive Bayes classifier on this data.\nTitanic Dataset\nThe Titanic dataset includes information on passengers such as class, gender, age, fare, and survivor status. Based on these characteristics, the purpose is to forecast whether or not a passenger survived.\nThe following are the steps involved in utilizing the Naive Bayes classifier method to predict Titanic survival:\n\nLoad the Titanic dataset.\nPreprocess the data by dealing with missing values, encoding category variables, and, if necessary, scaling numerical characteristics.\nDivide the data into two sets: training and testing.\nOn the training data, train a Naive Bayes classifier.\nAnalyze the classifier’s performance on the testing data using relevant measures like as accuracy, precision, recall, or F1 score.\nUsing the learned classifier, predict the survival result for new, unknown data.\n\nTo understand the procedure, let’s go over the code step by step.\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Graph plots\n\nimport seaborn as sns\n\n# Data splitting for training and testing\nfrom sklearn.model_selection import train_test_split \n# Importing the Guassian Naive Bayes model\nfrom sklearn.naive_bayes import GaussianNB \n\n# Printing options\nnp.set_printoptions(suppress=True, precision=6) \n\n# Read the titanic dataset\ndf = pd.read_csv(\"data/titanic-data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npassenger_id\nname\np_class\ngender\nage\nsib_sp\nparch\nticket\nfare\ncabin\nembarked\nsurvived\n\n\n\n\n0\n1\nBraund, Mr. Owen Harris\n3\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n0\n\n\n1\n2\nCumings, Mrs. John Bradley (Florence Briggs Th...\n1\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n1\n\n\n2\n3\nHeikkinen, Miss. Laina\n3\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n1\n\n\n3\n4\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\n1\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n1\n\n\n4\n5\nAllen, Mr. William Henry\n3\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n0\n\n\n\n\n\n\n\n\n# Dropping irrelevant columns\ndf.drop([\"passenger_id\", \"name\", \"sib_sp\", \"parch\", \"ticket\", \"cabin\", \"embarked\"], axis=1, inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nsurvived\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\n0\n\n\n1\n1\nfemale\n38.0\n71.2833\n1\n\n\n2\n3\nfemale\n26.0\n7.9250\n1\n\n\n3\n1\nfemale\n35.0\n53.1000\n1\n\n\n4\n3\nmale\n35.0\n8.0500\n0\n\n\n\n\n\n\n\n\n# Separating target and inputs\nexpected_target = df[\"survived\"] \ninputs = df.drop(\"survived\", axis=1)\n\nprint(expected_target.head())\nprint(inputs.head())\n\n0    0\n1    1\n2    1\n3    1\n4    0\nName: survived, dtype: int64\n   p_class  gender   age     fare\n0        3    male  22.0   7.2500\n1        1  female  38.0  71.2833\n2        3  female  26.0   7.9250\n3        1  female  35.0  53.1000\n4        3    male  35.0   8.0500\n\n\n\n# Converting gender column into dummy variables\ndummies = pd.get_dummies(inputs[\"gender\"]) \n\nprint(dummies.head())\nprint(dummies.dtypes)\n\n   female   male\n0   False   True\n1    True  False\n2    True  False\n3    True  False\n4   False   True\nfemale    bool\nmale      bool\ndtype: object\n\n\n\n# Concatenating inputs and dummies\ninputs = pd.concat([inputs, dummies], axis=1)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\ngender\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\nmale\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\nfemale\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\nfemale\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\nfemale\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\nmale\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\n\n# dropping the gender column because we now have the female and male columns.\ninputs.drop([\"gender\"], axis=1, inplace=True)\ninputs.head()\n\n\n\n\n\n\n\n\np_class\nage\nfare\nfemale\nmale\n\n\n\n\n0\n3\n22.0\n7.2500\nFalse\nTrue\n\n\n1\n1\n38.0\n71.2833\nTrue\nFalse\n\n\n2\n3\n26.0\n7.9250\nTrue\nFalse\n\n\n3\n1\n35.0\n53.1000\nTrue\nFalse\n\n\n4\n3\n35.0\n8.0500\nFalse\nTrue\n\n\n\n\n\n\n\nIn the above code chunks, we import the dataset, eliminate extraneous columns, and turn the gender column into dummy variables. Dummy variables are binary indicators that represent categories, such as ‘female’ and ‘male’ in this example.\n\n# Some columns contain null values\ninputs.age[:10]\n\n# We fill null values with the mean value of the whole colum\ninputs[\"age\"] = inputs[\"age\"].fillna(inputs[\"age\"].mean())\n\ninputs.age[:10]\n# 5th row null is replaced with the mean value.\n\n0    22.000000\n1    38.000000\n2    26.000000\n3    35.000000\n4    35.000000\n5    29.699118\n6    54.000000\n7     2.000000\n8    27.000000\n9    14.000000\nName: age, dtype: float64\n\n\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(inputs, expected_target, test_size=0.2)\nprint(len(X_train), len(X_test), len(inputs))\n\n712 179 891\n\n\n\n# calculating training and testing data percentage.\nprint(len(X_train) / len(inputs)) # training data %\nprint(len(X_test) / len(inputs)) # testing data %\n\n0.7991021324354658\n0.20089786756453423\n\n\nIn the above code chunks, we fill in missing values in the age field with the mean age. The data is then divided into training and testing sets, with 80% being utilized for training and 20% for testing.\n\n# Building and evaluating the Gaussian Naive Bayes model\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Evaluating the model's accuracy on the test set\nmodel.score(X_test, y_test)\n\n0.7653631284916201\n\n\nHere, we use the Gaussian Naive Bayes model, a probabilistic model suitable for classification tasks. We fit the model with the training data and evaluate its accuracy on the test set.\n\n# Predictions and probabilities\npred = np.array(model.predict(X_test))\npred_probability = np.array(model.predict_proba(X_test))\n\n# Displaying predictions and probabilities for the first 5 samples\nprint(pred[:5])\nfor i in range(1, 6):\n    print(pred_probability[i][0], end=\", \")\n\n[0 1 0 0 0]\n0.02328677203377912, 0.9783497697196583, 0.9887445318343112, 0.982116447034732, 0.033932836580610186, \n\n\nWe obtain predictions and probabilities for the test set in the above code chunk. The probabilities represent the likelihood of a passenger not surviving (in this case, the first class in pred_probability[i]).\nVisualization\n\n# Handling missing values in the age column for the entire dataset\ndf[\"age\"] = df[\"age\"].fillna(df[\"age\"].mean())\ndf.age[:10]\n\n# Creating age bins and labels\nage_bins = [0, 25, 40, 65, float('inf')]\nage_labels = ['0-25', '25-40', '40-65', '65+']\ndf[\"age\"] = pd.cut(df['age'], bins=age_bins, labels=age_labels)\n\n# Visualizing age distribution and survival rate\nfig, axs = plt.subplots(nrows=2)\n\nplt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4,hspace=0.4)\n\n#Histogram of Age of the given data set(sample)\nsns.countplot(df, x=\"age\", ax=axs[0]).set(title=\"Agewise distribution of the passenger aboard the Titanic\")\n\nsns.countplot(df, x=\"age\", hue=\"survived\", ax=axs[1]).set(title=\"Agewise distribution of survivors\")\n\n[Text(0.5, 1.0, 'Agewise distribution of survivors')]\n\n\n\n\n\nFinally, we handle missing values in the age column for the entire dataset, create age bins, and visualize the age distribution and survival rate."
  },
  {
    "objectID": "posts/Blog 1/index.html#conclusion",
    "href": "posts/Blog 1/index.html#conclusion",
    "title": "Blog 1: Probability theory and random variables",
    "section": "",
    "text": "In conclusion, probability theory and random variables are vital in machine learning, especially when dealing with uncertainty and making predictions. The code provided demonstrates a practical example using the Titanic dataset and a Gaussian Naive Bayes model, showcasing the steps involved in preprocessing data, building a model, and evaluating its performance."
  },
  {
    "objectID": "about.html#course-cs-5805-machine-learning",
    "href": "about.html#course-cs-5805-machine-learning",
    "title": "About",
    "section": "Course: CS 5805 Machine Learning",
    "text": "Course: CS 5805 Machine Learning\nThis is a five series blog on the following Machine Learning topics for the course CS 5805 at Virginia tech.\nTopics:\n\nProbability theory and random variables\nClustering\nLinear and nonlinear regression\nClassification\nAnomaly/outlier detection"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Blog 1: Probability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 2: Clustering\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 3: Linear and Non-linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4: Classification\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\n  \n\n\n\n\nBlog 5: Anomaly/outlier Detection\n\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nShambhavi Kuthe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Blog 2/index.html",
    "href": "posts/Blog 2/index.html",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "Clustering, a process that involves grouping related data points together, is one of the fundamental jobs in the large area of machine learning. Clustering is important in a variety of applications, from customer segmentation to image analysis. The K-Means algorithm, recognized for its simplicity and efficacy, is a strong clustering technique.\nUnderstanding Clustering\nClustering is a fundamental machine learning approach that involves grouping together comparable data points based on underlying patterns or similarities in the data. Clustering is classified as unsupervised learning since it acts on unlabeled datasets, as opposed to supervised learning, where the algorithm is taught on labeled data. Clustering’s primary purpose is to find and arrange data points into discrete clusters, with each cluster comprising components with similar properties. This approach aids in the discovery of hidden structures inside data, making it a useful tool for tasks such as customer segmentation, anomaly detection, and pattern identification. K-Means is a popular clustering technique that divides data into K-groups and assigns each data point to the cluster with the closest centroid. Clustering algorithms play an important role in exploratory data analysis, helping to extract useful insights from vast and complicated datasets.\nK-means Clustering\nK-Means clustering is a well-known unsupervised machine learning approach for partitioning a dataset into K separate, non-overlapping subsets or clusters. Iteratively, the method assigns each data point to the cluster with the closest centroid (mean point), then updates the centroids depending on the newly allocated points. The number of clusters, K, is a user-specified parameter. K-Means is especially successful in situations where the data shows clear patterns of separation, and its simplicity and effectiveness make it popular for applications like customer segmentation, picture reduction, and anomaly identification. The algorithm’s success is due to its ability to rapidly converge on a solution, which makes it computationally efficient for huge datasets. It is, however, sensitive to the initial placement of centroids, and the value of K can have a substantial influence on the quality of the clustering results.\n\n\nLet’s look at the Python code that shows how to use K-Means clustering on customer data. The dataset utilized in this case is from a shopping mall, and the goal is to classify clients based on their yearly income and age.\nDataset:\nThe dataset was obtained from Kaggle : https://www.kaggle.com/nelakurthisudheer/mall-customer-segmentation\nThe dataset contains the following five characteristics of 200 customers :\n\nCustomerID: A unique identifier issued to the customer.\nGender: The customer’s gender\nAge: The customer’s age.\nAnnual Income (k$): The customer’s annual income. The mall assigns a spending score (1-100) based on client behavior and spending habits.\n\n\n# Importing libraries and reading the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading the dataset\ncustomers = pd.read_csv('data\\Mall_Customers.csv')\nprint(customers.head()) \n\n# Checking the summary statistics\ncustomers.describe()\n\n   CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Annual Income (k$)'])\n\n&lt;Axes: xlabel='Annual Income (k$)', ylabel='Density'&gt;\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Age'])\n\n&lt;Axes: xlabel='Age', ylabel='Density'&gt;\n\n\n\n\n\nImporting the appropriate libraries, loading the dataset, and studying its fundamental statistics and feature distributions are the first stages.\n\n\nThe elbow method is used to estimate the optimal number of clusters in the first clustering job, which is dependent on yearly income.\n\ndata = customers.iloc[:,[3,4]].values\nsos = []#within cluster sum of squares\n\n# Applying K-Means with different cluster numbers\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)# inertia_ = to find the sos value\n\n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method assists in determining the optimal number of clusters by noting when the within-cluster sum of squares (SOS) begins to plateau. The ‘elbow’ appears at k=5, suggesting that there are five unique clusters in this example.\n\n# Applying K-Means with k=5\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n#plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\nax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100, c='green', label='Cluster 5')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nThe above code segment depicts the clusters and centroids found based on yearly income and spending score.\n\n\n\nThe same process is repeated for clustering based on age.\n\n# Extracting relevant data\ndata=customers.iloc[:,[2,4]].values\n\n# Elbow method for determining optimal clusters\nsos = []\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)\n    \n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method is used once more to determine the ideal number of clusters. In this example, the code suggests that k=4 is a good choice.\n\n# Applying K-Means with k=4\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n# Plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Age')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nAge and expenditure score are used to depict clusters and centroids.\n\n\n\n\nIn this blog article, we looked at clustering, a strong unsupervised learning approach. We applied clustering to customer data using the K-Means technique, revealing separate groupings based on yearly income and age. The elbow approach was critical in finding the ideal amount of clusters, and the visualizations that resulted offered insights into consumer segmentation. Understanding and utilizing clustering techniques such as K-Means may have far-reaching consequences in a variety of fields, providing useful insights and paving the way for better informed decision-making."
  },
  {
    "objectID": "posts/Blog 2/index.html#the-code-breakdown",
    "href": "posts/Blog 2/index.html#the-code-breakdown",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "Let’s look at the Python code that shows how to use K-Means clustering on customer data. The dataset utilized in this case is from a shopping mall, and the goal is to classify clients based on their yearly income and age.\nDataset:\nThe dataset was obtained from Kaggle : https://www.kaggle.com/nelakurthisudheer/mall-customer-segmentation\nThe dataset contains the following five characteristics of 200 customers :\n\nCustomerID: A unique identifier issued to the customer.\nGender: The customer’s gender\nAge: The customer’s age.\nAnnual Income (k$): The customer’s annual income. The mall assigns a spending score (1-100) based on client behavior and spending habits.\n\n\n# Importing libraries and reading the data\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Loading the dataset\ncustomers = pd.read_csv('data\\Mall_Customers.csv')\nprint(customers.head()) \n\n# Checking the summary statistics\ncustomers.describe()\n\n   CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n\n\n\n\n\n\n\n\n\nCustomerID\nAge\nAnnual Income (k$)\nSpending Score (1-100)\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n100.500000\n38.850000\n60.560000\n50.200000\n\n\nstd\n57.879185\n13.969007\n26.264721\n25.823522\n\n\nmin\n1.000000\n18.000000\n15.000000\n1.000000\n\n\n25%\n50.750000\n28.750000\n41.500000\n34.750000\n\n\n50%\n100.500000\n36.000000\n61.500000\n50.000000\n\n\n75%\n150.250000\n49.000000\n78.000000\n73.000000\n\n\nmax\n200.000000\n70.000000\n137.000000\n99.000000\n\n\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Annual Income (k$)'])\n\n&lt;Axes: xlabel='Annual Income (k$)', ylabel='Density'&gt;\n\n\n\n\n\n\n# Visualizing data distributions\nsns.distplot(customers['Age'])\n\n&lt;Axes: xlabel='Age', ylabel='Density'&gt;\n\n\n\n\n\nImporting the appropriate libraries, loading the dataset, and studying its fundamental statistics and feature distributions are the first stages.\n\n\nThe elbow method is used to estimate the optimal number of clusters in the first clustering job, which is dependent on yearly income.\n\ndata = customers.iloc[:,[3,4]].values\nsos = []#within cluster sum of squares\n\n# Applying K-Means with different cluster numbers\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)# inertia_ = to find the sos value\n\n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method assists in determining the optimal number of clusters by noting when the within-cluster sum of squares (SOS) begins to plateau. The ‘elbow’ appears at k=5, suggesting that there are five unique clusters in this example.\n\n# Applying K-Means with k=5\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n#plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\nax.scatter(data[y_kmeans==4,0],data[y_kmeans==4,1],s=100, c='green', label='Cluster 5')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Annual Income (k$)')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nThe above code segment depicts the clusters and centroids found based on yearly income and spending score.\n\n\n\nThe same process is repeated for clustering based on age.\n\n# Extracting relevant data\ndata=customers.iloc[:,[2,4]].values\n\n# Elbow method for determining optimal clusters\nsos = []\nfor i in range(1, 11):\n    kmeans=KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(data)\n    sos.append(kmeans.inertia_)\n    \n# Plotting the Elbow Method\nplt.plot(range(1, 11), sos)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('SOS')\nplt.show()\n\n\n\n\nThe Elbow Method is used once more to determine the ideal number of clusters. In this example, the code suggests that k=4 is a good choice.\n\n# Applying K-Means with k=4\nkmeans=KMeans(n_clusters=5, init='k-means++', random_state=0)\ny_kmeans=kmeans.fit_predict(data)\n\n# Plotting the clusters\nfig,ax = plt.subplots(figsize=(8, 4))\n\n# Scatter plot for each cluster\nax.scatter(data[y_kmeans==0,0],data[y_kmeans==0,1],s=100,c='magenta',label='Cluster 1')\nax.scatter(data[y_kmeans==1,0],data[y_kmeans==1,1],s=100, c='cyan', label='Cluster 2')\nax.scatter(data[y_kmeans==2,0],data[y_kmeans==2,1],s=100, c='red', label='Cluster 3')\nax.scatter(data[y_kmeans==3,0],data[y_kmeans==3,1],s=100, c='blue', label='Cluster 4')\n\n# Plotting centroids\nax.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s=200,c='yellow',label='Centroid')\n\nplt.title('Cluster Segmentation of Customer')\nplt.xlabel('Age')\nplt.ylabel('Spending Score(1 - 100)')\nplt.legend()\nplt.show()\n\n\n\n\nAge and expenditure score are used to depict clusters and centroids."
  },
  {
    "objectID": "posts/Blog 2/index.html#conclusion",
    "href": "posts/Blog 2/index.html#conclusion",
    "title": "Blog 2: Clustering",
    "section": "",
    "text": "In this blog article, we looked at clustering, a strong unsupervised learning approach. We applied clustering to customer data using the K-Means technique, revealing separate groupings based on yearly income and age. The elbow approach was critical in finding the ideal amount of clusters, and the visualizations that resulted offered insights into consumer segmentation. Understanding and utilizing clustering techniques such as K-Means may have far-reaching consequences in a variety of fields, providing useful insights and paving the way for better informed decision-making."
  }
]